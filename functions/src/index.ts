import {genkit, z} from "genkit";
import {googleAI} from "@genkit-ai/google-genai";
import { https } from "firebase-functions/v2";

// Gemini Developer API models and Vertex Express Mode models depend on an API key.
// API keys should be stored in Cloud Secret Manager so that access to these
// sensitive values can be controlled. defineSecret does this for you automatically.
// If you are using Google Developer API (googleAI) you can get an API key at https://aistudio.google.com/app/apikey
// If you are using Vertex Express Mode (vertexAI with apiKey) you can get an API key
// from the Vertex AI Studio Express Mode setup.
import { defineSecret } from "firebase-functions/params";
const apiKey = defineSecret("GOOGLE_GENAI_API_KEY");

// The Firebase telemetry plugin exports a combination of metrics, traces, and logs to Google Cloud
// Observability. See https://firebase.google.com/docs/genkit/observability/telemetry-collection.
import {enableFirebaseTelemetry} from "@genkit-ai/firebase";
enableFirebaseTelemetry();

const ai = genkit({
  plugins: [
    // Load the GoogleAI provider. You can optionally specify your API key by
    // passing in a config object; if you don't, the provider uses the value
    // from the GOOGLE_GENAI_API_KEY environment variable, which is the
    // recommended practice.
    googleAI()
  ],
});

// Define a simple flow that prompts an LLM to generate menu suggestions.
const menuSuggestionFlow = ai.defineFlow({
    name: "menuSuggestionFlow",
    inputSchema: z.string().describe("A restaurant theme").default("seafood"),
    outputSchema: z.string(),
    streamSchema: z.string(),
  }, async (subject, { sendChunk }) => {
    // Construct a request and send it to the model API.
    const prompt =
      `Suggest an item for the menu of a ${subject} themed restaurant`;
    const { response, stream } = ai.generateStream({
      model: "googleai/gemini-1.5-flash",
      prompt: prompt,
      config: {
        temperature: 1,
      },
    });

    for await (const chunk of stream) {
      sendChunk(chunk.text);
    }

    // Handle the response from the model API. In this sample, we just
    // convert it to a string, but more complicated flows might coerce the
    // response into structured output or chain the response into another
    // LLM call, etc.
    return (await response).text;
  }
);

export const menuSuggestion = https.onCall({ secrets: [apiKey] }, async (request) => {
    return await menuSuggestionFlow(request.data);
});
